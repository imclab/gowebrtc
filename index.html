<!--
Google IO 2012/2013 HTML5 Slide Template

Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mah√© <lukem@google.com>

URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title></title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">-->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0">-->
  <!--This one seems to work all the time, but really small on ipad-->
  <!--<meta name="viewport" content="initial-scale=0.4">-->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides" src="js/require-1.0.8.min.js"></script>
</head>
<body style="opacity: 0">

<slides class="layout-widescreen">

<slide class="logoslide nobackground">
  <article class="flexbox vcenter">
    <div style='margin: 0 0 2em 0'><img src="images/google_developers_logo.png"></div>
<!--     <div>Move forward and back with the arrow keys</div>
 -->  </article>
</slide>

<slide class="title-slide segue nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
  <hgroup class="auto-fadein">
    <h1 data-config-title><!-- populated from slide_config.json --></h1>
    <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
    <p data-config-presenter><!-- populated from slide_config.json --></p>
  </hgroup>
  <aside class="note"><p>
  </p></aside>
</slide>

<!-- <slide class="nobackground">
  <hgroup>
    <h2>Watch this presentation on YouTube</h2>
  </hgroup>
  <article>
    <iframe  width="730" style="width: 730px;" src="http://www.youtube.com/embed/xxxxxx"></iframe>
  </article>
</slide> -->

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'><a href="http://www.samdutton.net/realtime2013" title="These slides online">gowebrtc.appspot.com</a></div>
    </article>
    <aside class="note">
    </aside>
</slide>

<slide class="segue dark quote nobackground">
  <aside class="gdbar right bottom"><img src="images/google_developers_icon_128.png"></aside>
  <article class="flexbox vleft auto-fadein">
    <q>
      WebRTC is a new front in the long war for an open and unencumbered web
    </q>
    <div class="author">
      Brendan Eich<br>
      &ndash; Mozilla CTO and inventor of JavaScript
    </div>
  </article>
  <aside class="note">
    <p>
   </p>
  </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div>Real-time communication built into the browser</div>
    </article>
    <aside class="note">
    <p>WebRTC is an open source project to enable real time communication to be built into web browsers. </p>
   </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div>Open source, no plugins, free</div>
    </article>
    <aside class="note">
      <p></p>
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div>Video, audio, data</div>
    </article>
    <aside class="note">
      <p></p>
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div>High quality, low cost</div>
    </article>
    <aside class="note">
      <p></p>
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'>Peer to peer</div>
    </article>
    <aside class="note">
      <p>And for reasons that will become obvious...</p>
      <p>WebRTC had to be peer to peer.</p>
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'><img src="images/no.png" alt="RTC via a server" style='max-width: 100%' /></div>
    </article>
    <aside class="note">
      <p>In the past, RTC worked like this.</p>
      <p>Noone wants communication based on a nasty diagram like that...</p>
      <p>It's inefficient and expensive running a server to relay media.</p>
    </aside>
</slide>


<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'><img src="images/yes.png" alt="RTC peer to peer" style='max-width: 100%' /></div>
    </article>
    <aside class="note">
      <p>This looks much better.</p>
      <p>it's got a nice fluffy cloud right in the middle.</p>
      <p>As we'll find out, if only it were that simple...</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Shopping list</h2>
  </hgroup>
  <article>
    <ul>
      <li>Protocols for communication: IETF</li>
      <li>Standards for APIs: W3C</li>
      <li>Media and communication stack: libjingle, VP8, Opus...</li>
      <li>Design for a new communications ecosystem</li>
    </ul>
  </article>
  <aside class="note">
    <p>A word about libjingle... Jingle is an extension of XMPP created by Google to enable audio and video for messaging. Libjingle is a C++ library, an implementation of Jingle initially developed for Google Talk.</p>
    <p>A new ecosystem of WebRTC-enabled devices.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>WebRTC across platforms</h2>
  </hgroup>
  <article style="height: 100%; position: relative">
    <ul class="tight" xstyle='margin: 0 1em 0 0'>
      <li>Chrome and Chrome for Android</li>
      <li>Firefox and Firefox for Android</li>
      <li>Opera</li>
      <li>Native <a href="https://code.google.com/p/libjingle/source/browse/trunk/talk/app/webrtc/java/src/org/webrtc/PeerConnection.java">Java</a> and Objective-C bindings</li>
    </ul>
    <img src="images/android.jpg" alt="Firefox/Chrome interoperability" style="position: absolute; right: 2em; top: -82px; width: 320px" />
    <img src="images/firefoxChrome.jpg" alt="Firefox/Chrome interoperability" style="position: absolute; bottom: 5em; width: 45%;" />
  </article>
  <aside>
    <p>Where are we now?</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Qt moving to Chromium</h2>
  </hgroup>
  <article>
    <ul>
      <li>Framework for cross-platform/device native and embedded apps</li>
      <li>Qt WebKit => Qt WebEngine</li>
      <li>Multimedia and new HTML5 features such as WebRTC working out-of the-box</li>
    </ul>
  </article>
  <aside class="note">
    <p>The Qt framework (for building cross-platform/device native and embedded apps, now owned by Digia) is moving from Qt WebKit to Qt WebEngine, based on Chromium</p>
  </aside>
</slide>

<slide class="nobackground">
  <article class="fill flexbox vcenter">
       <div class="big"><b>1,000,000,000+</b></div>
       <div>WebRTC Endpoints</div>
  </article>
  <aside class="note">
  <p>Over one billion WebRTC-enabled users, which gives some idea of the size of this opportunity. In order to grow the ecosystem further, we're also providing official, supported native versions of WebRTC for Android, and very soon iOS.</p>
  </aside>
</slide>

<slide class="segue dark quote nobackground">
  <aside class="gdbar right bottom"><img src="images/google_developers_icon_128.png"></aside>
  <article class="flexbox vleft auto-fadein">
    <q>Voice is just another JS application</q>
    <div class="author">
      Henning Schulzrinne<br>
      &ndash; CTO, US FCC
    </div>
  </article>
  <aside class="note">
    <p>The impact could be profound.</p>
    <p>The end of telephony as we know it? Time will tell</p>
  </aside>
</slide>

<slide class="nobackground">
  <article class="fill flexbox vcenter">
    <div><a href="blog.vline.com/post/61581986806/live-tv-interview-powered-by-vline-customer-in-quality" title="vLine blog post" style="border-bottom: none;"><img src="images/skyLarge.jpg" alt="Sky TV interview done via WebRTC" style="width: 100%;" /></a></div>
  </article>
  <aside class="note">
  <p>Video chat, games, messaging -- imagine a scientific app using sensors on a low spec device running Chrome, collaborative editing suite for broadcast</p>
  <p>Just last month saw the first live TV interview done via WebRTC</p>
  </aside>
</slide>

<slide class="nobackground">
  <article class="fill flexbox vcenter">
    <div><a href="blog.vline.com/post/61581986806/live-tv-interview-powered-by-vline-customer-in-quality" title="vLine blog post" style="border-bottom: none;"><img src="images/skyKit.jpg" alt="Webcam and Yeti mic used for Sky interview" style="width: 100%;" /></a></div>
  </article>
  <aside class="note">
  <p>And the kit to do that?</p>
  <p>A hundred Euro Yeti mic and I webcam like the one I've got here</p>
  </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'><img src="images/obTrucks.jpg" alt="Outdoor broadcast truck" style='width: 100%' /></div>
      <footer class="source" style='position: absolute; bottom: 23px; left: 150px'><a href="https://en.wikipedia.org/wiki/File:BBC_HD_SNG.jpg" title="OB trucks">en.wikipedia.org/wiki/File:BBC_HD_SNG.jpg</a></footer>
    </article>
    <aside class="note">
      <p>You don't need one of these!</p>
    </aside>
</slide>


<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>What do we need for RTC?</h2>
    <h3></h3>
  </hgroup>
  <aside class="note">
    <p>So that's the vision for WebRTC. Now let's dig into the APIs that WebRTC provides.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Three main tasks</h2>
  </hgroup>
    <article>
  <ul>
    <li>Acquiring audio and video</li>
    <li>Communicating audio and video</li>
    <li>Communicating arbitrary data</li>
  </ul>
    </article>
    <aside class="note">
    <p></p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Three main JavaScript APIs</h2>
  </hgroup>
  <article>
  <ul>
    <li>MediaStream (aka getUserMedia)</li>
    <li>RTCPeerConnection</li>
    <li>RTCDataChannel</li>
  </ul>
  </article>
  <aside class="note">
  </aside>
</slide>

<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>MediaStream</h2>
    <h3>Acquiring audio and video</h3>
  </hgroup>
  <aside class="note">
    <p>Prime use case for WebRTC is... communication.</p>
    <p>Prime example of that is voice and video chat.</p>
    <p>What do we need to do for that?</p>
    <p></p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>What do we need?</h2>
  </hgroup>
  <article>
    <ul>
      <li>Streaming media: <code>MediaStream</code></li>
      <li>Get local media: <code>navigator.getUserMedia()</code></li>
    </ul>
  </article>
  <aside class="note">
    <p>Need to work with streaming media.</p>
    <p>The shopping list is getting longer...</p>
    <p>A way to get local media.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>MediaStream</h2>
  </hgroup>
  <article>
    <ul>
      <li>Represents a stream of audio and/or video</li>
      <li>Can contain multiple 'tracks'</li>
      <li>Obtain a MediaStream with <code>navigator.getUserMedia()</code></li>
    </ul>
  </article>
  <aside class="note">
  </aside>
</slide>

<slide class="nobackground">
  <article class="flexbox vcenter">
    <img src="images/mediaStream.png" alt="MediaStream diagram" style="width: 733px" />
  </article>
  </article>
  <aside class="note">
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>gUM</h2>
    <h3>It's pretty simple.</h3>
  </hgroup>
  <article>
    <pre class="prettyprint" data-lang="javascript">
var constraints = {video: true};

function successCallback(stream) {
  var video = document.querySelector("video");
  video.src = window.URL.createObjectURL(stream);
}

function errorCallback(error) {
  console.log("navigator.getUserMedia error: ", error);
}

<b>navigator.getUserMedia(constraints, successCallback, errorCallback);</b>
</pre>
  </article>
</slide>


<slide class="nobackground">
  <article class="fill flexbox vcenter">
    <div class="bigger"><a href="http://www.simpl.info/gum" title="Simple getUserMedia demo">simpl.info/gum</a></div>
  </article>
  <aside class="note">
    <p>Chrome, Firefox, Chrome and Firefox on Android.</p>
    <p>Not Opera right now -- which is in transition.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>gUM permissions</h3>
  </hgroup>
  <article>
    <ul>
      <li>HTTPS</li>
      <li>Chrome apps: <code>audioCapture</code> and <code>videoCapture</code> permissions</li>
      <li>Chrome flag: <code>--use-fake-ui-for-media-stream</code></li>
      <li>UI settings can be changed afterwards.</li>
    </ul>
  </article>
  <aside class="note">
   <p>If you run with HTTPS, or from an app, permission will only be asked for once.</p>
    <p>Likewise with Chrome apps that ask for "audioCapture" and"videoCapture" permissions. Permission is only requested on installation.</p>
    <p>Chrome flag: <code>--use-fake-ui-for-media-stream</code></p>
    <p>UI settings can be changed afterwards.</p>
  </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'><img src="images/file.png" alt="Don't use file:// URLs" style='max-width: 100%' /></div>
    </article>
    <aside class="note">
      <p>A word of warning: you must run getUserMedia() from a server.</p>
      <p>Otherwise you'll get a rather baffling PERMISSION_DENIED error.</p>
      <p>There are efforts underway to provide better error messages.</p>
    </aside>
</slide>

<!-- <slide class="nobackground">
  <hgroup>
    <h2>Multiple inputs</h2>
  </hgroup>
  <article class="fill flexbox vcenter">
    <div id='multipleInputs'>
    <img id='nexusInput' src="images/nexus10.png" alt="Nexus 10" />
    <div>
    Microphone<br />
    ‚Üí<br />
    Front camera<br />
    ‚Üí<br />
    Rear camera<br />
    ‚Üí<br />
    App sharing video<br />
    ‚Üí<br />
    <br />
    Webcam video<br />
    ‚Üê<br />
    Stereo audio<br />
    ‚Üê
    </div>
  <img id='pixelInput' src="images/pixel.jpg" alt="Chromebook Pixel" />
  </div>
  </article>
  <aside class="note">
    <p>Chrome, Firefox, Opera, Chrome and Firefox on Android.</p>
  </aside>
</slide> -->

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://idevelop.github.com/ascii-camera/" title="getUserMedia video rendered as ASCII art">idevelop.github.com/ascii-camera</a></div>
    </article>
    <aside class="note">
    </aside>
</slide>

<!-- <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://lli.web.fh-koeln.de/mocowe" title="getUserMedia used to control a slide deck">lli.web.fh-koeln.de/mocowe</a></div>
    </article>
    <aside class="note">
    </aside>
</slide> -->

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://www.shinydemos.com/facekat/" title="getUserMedia used to control a game">FaceKat</a></div>
    </article>
    <aside class="note">
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://webcamtoy.com/app" title="getUserMedia photobooth, with effects">webcamtoy.com</a></div>
    </article>
    <aside class="note">
    </aside>
</slide>

<!-- <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://www.soundstep.com/blog/experiments/jsdetection/" title="getUserMedia xylophone">soundstep.com/blog/experiments/jsdetection</a></div>
    </article>
    <aside class="note">
    </aside>
</slide> -->

<!-- <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://www.simpl.info/headtrackr" title="getUserMedia with headtrackr.js face detection">simpl.info/headtrackr</a></div>
    </article>
    <aside class="note">
    <p>Check out the console for headtracking events!</p>
    </aside>
</slide> -->


<slide>
  <hgroup>
    <h2>Constraints</h2>
  </hgroup>
  <article>
    <ul class="tight">
      <li>Mandatory or optional</li>
      <li>Resolution: width and height</li>
      <ul class="tight">
      <li>from a <a href="https://code.google.com/p/chromium/issues/detail?id=143631#c9" title="Constraints resolutions">fixed list</a></li>
      <li>no cropping or scaling (yet)</li>
      </ul>
      <li>Frame rate</li>
      <li>Facing mode: front or back camera</li>
      <li>Source type: video camera, screen capture...</li>
      <li>Source id</li>
      <li>Volume</li>
    </ul>
  </article>
  <aside class="note">
    <p>Constraints allow us to impose constraints on media</p>
    <p>In the simple gUM example, we chose to get video only, so as to avoid feedback.</p>
  </aside>
</slide>

<slide>
  <hgroup>
  </hgroup>
    <article class="fill flexbox vcenter">
      <div><a href="https://simpl.info/getusermedia/constraints/" title="getUserMedia constraints demo">simpl.info/getusermedia/constraints</a></div>
    </article>
    <aside class="note">
      <p>Constraints can let us choose resolution.</p>
      <p>Note that scaling and cropping aren't supported by browsers.</p>
    </aside>
</slide>

<slide>
  <hgroup>
  </hgroup>
    <article class="fill flexbox vcenter">
      <div><a href="https://simpl.info/getusermedia/sources/" title="getUserMedia constraints demo">simpl.info/getusermedia/sources
</a></div>
    </article>
    <aside class="note">
      <p>Constraints also let us choose media source.</p>
      <p>[Run, plug in camera, reload page, the show on Android Beta.]</p>
      <p>By the way, if you want to install Chrome Beta on your Android</p>
    </aside>
</slide>


<slide>
  <hgroup>
    <h2>Facing mode, applyConstraints()</h2>
  </hgroup>
  <article>
    <ul>
      <li>Choose source: <a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html#idl-def-VideoFacingModeEnum" title="W3C facing mode draft spec">spec</a></li>
      <li>Apply constraints from JavaScript: <a href="http://dev.w3.org/2011/webrtc/editor/getusermedia.html#widl-MediaStreamTrack-applyConstraints-void-MediaTrackConstraints-constraints" title="W3C applyConstraints() draft spec">spec</a></li>
    </ul>
  </article>
  <aside class="note">
    <p>With facing mode: </p>
    <p>With applyConstraints(): width, height, framerate, facingMode, etc.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>getUserMedia + Web Audio</h2>
  </hgroup>
  <article>
    <pre class="prettyprint" data-lang="javascript">
// Success callback when requesting audio input stream
function gotStream(stream) {
    var audioContext = new webkitAudioContext();

    // Create an AudioNode from the stream
    var mediaStreamSource = audioContext.createMediaStreamSource(stream);

    // Connect it to the destination or any other node for processing!
    mediaStreamSource.connect(audioContext.destination);
}

navigator.webkitGetUserMedia({audio:true}, gotStream);
</pre>
  <p style='margin: 2em 0 0 0'>Make sure to enable Web Audio Input in about:flags!</p>

  </article>
  <aside class="note">
  <p>RTCPeerConnection will also accept Web Audio output.</p>
  </aside>
</slide>

 <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://www.webaudiodemos.appspot.com/AudioRecorder/index.html" title="Record audio">webaudiodemos.appspot.com/AudioRecorder</a></div>
    </article>
    <aside class="note">
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>gUM screencapture</h2>
  </hgroup>
  <article>
    <pre class="prettyprint" data-lang="javascript">
var constraints = {
  video: {
    mandatory: {
      chromeMediaSource: 'screen'
    }
  }
};

navigator.webkitGetUserMedia(constraints, gotStream);
</pre>
<!-- <a href="https://simpl.info/screencapture/" title="Screen capture‚ÄìRTCPeerConnection demo">simpl.info/screencapture</a> -->
  </article>
  <aside class="note">
    <p>Tab capture is also available from Chrome apps.</p>
  </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div style='margin: 0 0 2em 0'><a href="https://html5-demos.appspot.com/static/getusermedia/screenshare.html" title="Screen sharing demo">Screen sharing</a></div>
      <div><a href="http://updates.html5rocks.com/2012/12/Screensharing-with-WebRTC" title="HTML5 Rocks update demoing tab capture">Tab capture: chrome.tabCapture</a></div>
    </article>
    <aside class="note">
    <p>Extremely useful for doing IT support for your extended family!</p>
    </aside>
</slide>

<!-- <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://webaudiodemos.appspot.com/pitchdetect/index.html" title="Pitch detection demo">webaudiodemos.appspot.com/pitchdetect</a></div>
    </article>
    <aside class="note">
    </aside>
</slide> -->

<!-- <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://www.webaudiodemos.appspot.com/input/index.html" title="Web Audio effects demo">webaudiodemos.appspot.com/input</a></div>
    </article>
    <aside class="note">
    </aside>
</slide> -->

<!-- <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://www.lab.aerotwist.com/webgl/audio-room" title="WebGL example">lab.aerotwist.com/webgl/audio-room</a></div>
    </article>
    <aside class="note">
    </aside>
</slide> -->

<!-- <slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div><a href="http://samdutton.net/backwards/" title="Record audio and play it backwards">samdutton.net/backwards</a></div>
    </article>
    <aside class="note">
    </aside>
</slide>

 -->

<!-- <slide>
  <hgroup>
    <h2>gUM ‚òû Web Audio ‚òû RTCPeerConnection</h2>
  </hgroup>
  <article>
  <p style="margin: 0 0 2em 0">Capture microphone input and stream it to a peer with processing applied:</p>
    <pre class="prettyprint" data-lang="javascript">
navigator.getUserMedia('audio', gotAudio);
function gotAudio(stream) {
  var microphone = context.createMediaStreamSource(stream);
  var filter = context.createBiquadFilter();
  var peer = context.createMediaStreamDestination();
  microphone.connect(filter);
  filter.connect(peer);
  peerConnection.addStream(peer.stream);
}
</pre>

  <p style="margin: 2em 0 0 0"><a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/webrtc-integration.html" title="W3C examples adapted from the MediaStream Processing API proposal">More Media Stream integration examples</a></p>

  </article>
  <aside class="note">
    <p>Already in experimental builds and working well.</p>
    <p>This is very powerful: effects into and out of RTCPeerConnection.</p>
    <p>'Join the boxes' architecture.</p>
  </aside>
</slide> -->

<slide>
  <hgroup>
    <h2>Media Stream Recording API</h2>
  </hgroup>
    <article>
    <ul>
      <li>Demo: <a href="http://simpl.info/mediarecorder" title="Media Stream Recording demo">simpl.info/mediarecorder</a></li>
      <li><a href="https://dvcs.w3.org/hg/dap/raw-file/default/media-stream-capture/MediaRecorder.html" title="W3C MediaRecorder draft spec">Spec</a></li>
      <li>Chrome <a href="https://groups.google.com/a/chromium.org/forum/?fromgroups=#!topic/blink-dev/2l_G_apqk30" title="blink-dev Media Stream Recording API Intent to Implement discussion">Intent to Implement</a></li>
      <li><a href="http://www.w3.org/TR/streams-api/" title="W3C Streams API draft spec">Streams API</a></li>
    </ul>
    </article>
    <aside class="note">
      <p>How does it work? A MediaRecorder is created which takes an audio stream from navigator.getUserMedia(). When a blob of recorded data becomes available (set to occur after two seconds) this is used to set the src of the audio element, using window.URL.createObjectURL().</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Media Stream Image Capture API</h2>
  </hgroup>
    <article>
    <ul>
      <li><del>Demo</del></li>
      <li><a href="http://gmandyam.github.io/image-capture/" title="W3C Media Stream Image Capture draft spec">Spec</a></li>
      <li><code>getFrame()</code> creates an <code>ImageData</code> object available in <code>onframegrab</code></li>
      <li><code>takePhoto()</code> creates a Blob available in <code>onphoto</code></li>
    </ul>
    </article>
    <aside class="note">
      <p>How does it work? A MediaRecorder is created which takes an audio stream from navigator.getUserMedia(). When a blob of recorded data becomes available (set to occur after two seconds) this is used to set the src of the audio element, using window.URL.createObjectURL().</p>
    </aside>
</slide>

<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>RTCPeerConnection</h2>
    <h3>Audio and video communication between peers</h3>
  </hgroup>
  <aside class="note">
    <p>This is the API for audio and video communication, to create a connection between peers.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Communicate Media Streams</h2>
  </hgroup>
  <article class="fill flexbox vcenter">
    <div>
      <img style="float: left; width: 27%;" src="images/caller.jpg" alt="WebRTC video chat: caller" />
      <div style="float: left; line-height: 2em; margin: 0 2em 0 2em; text-align: center;">
      ‚Üí<br />
      getUserMedia<br />
      +<br />
      RTCPeerConnection<br />
      ‚Üê
      </div>
      <img  style="float: left; position: relative; top: 38px; width: 35%;" src="images/callee.jpg" alt="WebRTC video chat: callee" />
    </div>
  </article>
  <aside class="note">
    On the surface, the API is simple - get access to MediaStreams via getUserMedia, then plug them into a PeerConnection, and they will get sent to another WebRTC endpoint automatically. And when we receive media from the remote side, this goes into new MediaStreams that can be rendered in our web page.
  </aside>
</slide>

<slide class="nobackground">
  <hgroup>
    <h2>RTCPeerConnection does a lot</h2>
  </hgroup>
    <article>
  <ul>
    <li>Signal processing</li>
    <li>Codec handling</li>
    <li>Peer to peer communication</li>
    <li>Security</li>
    <li>Bandwidth management</li>
  </ul>
    <p>...</p>
    </article>
    <aside class="note">
    <p>Under the hood though, RTCPeerConnection is doing a lot - processing audio and video to remove noise, compressing the data using codecs, setting up the peer to peer pathway through NATs and firewalls, encrypting the data, ensuring we use the right amount of bandwidth...</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>WebRTC architecture</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/webrtcArchitecture.png" alt="WebRTC architecture diagram" />
  </article>
  <aside class="note">
  <p>There's a lot of moving parts under the hood. Fortunately, with RTCPeerConnection, this is mostly abstracted away. You create a RTCPeerConnection, add your own MediaStreams to it, call a couple methods to set up the right parameters for the call, and off you go. Sam's going to now show us a super-simple example of RTCPeerConnection.'
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>RTCPeerConnection initialisation</h2>
  </hgroup>
  <article>
    <ul>
      <li>Ascertain local media conditions: resolution, codec capabilities...</li>
      <li>Get potential network addresses for the application's host: candidates</li>
    </ul>
  </article>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div>Peer to peer &mdash; but we need servers  :^\</div>
    </article>
    <aside class="note">
      <p>So discovery and Signaling require intermediary servers to set up a call.</p>
      <p>At this point in history there's no way to shout into the Internet, 'Exchange streaming data with my friend's computer!'</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>What does WebRTC need servers for?</h2>
  </hgroup>
  <article>
    <ul>
      <li>Exchange metadata to coordinate communication: signaling</li>
      <li>Cope with NATs and firewalls: STUN and TURN</li>
    </ul>
  </article>
</slide>

<slide>
  <hgroup>
    <h2>JSEP architecture</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/jsep.png" alt="JSETP architecture diagram">
  </article>
    <aside class="note">
    <p>Here's a diagram that demonstrates this process. The caller sends a session description to its signaling server in the cloud, which then forwards this on to the callee. Similarly, the callee then sends its own session description back through the cloud to the caller. Once each side has given the session descriptions to RTCPeerConnection, the peer-to-peer link is established and media can flow.</p>
  </aside>
</slide>

<slide class="nobackground">
  <hgroup>
    <h2>Signaling</h2>
  </hgroup>
  <article>
    <ul>
      <li>Need to exchange 'session description' objects:</li>
      <ul class="tight">
        <li>What media formats I support, what I want to send</li>
        <li>Network information for peer-to-peer setup</li>
      </ul>
      <li>Can use any messaging mechanism</li>
      <li>Can use any messaging protocol</li>
      <li>...and can be used for application data</li>
    </ul>
  </article>
  <aside class="note">
    <p>Signaling is the process of coordinating communication. Similar to, when you make a phone call, the phone system sends a message to the person you're calling indicating that there's an incoming call. Then, when you answer the call, a message is sent back indicating the call is live.</p>
    <p>The same is true for WebRTC. A message needs to get sent by each side indicating the parameters they want to use for the call. This is called a "session description", and it includes a bunch of details regarding codecs, encryption, network information, etc.</p>
    <p>The details aren't critical for most apps; they just need to exchange these messages in some way. The mechanism is up to the app - it can use WebSocket, XHR and Server-sent Events, whatever it wants to use.</p>
    <p>Similarly, the exact protocol through which these are exchanged is also up to the app - many apps will send these messages as JSON, although some apps may use the standard SIP or XMPP protocols.</p>
  </aside>
</slide>



<slide>
  <hgroup>
    <h2>Signaling: how?</h2>
  </hgroup>
  <article>
  <ul>
    <li>Needs to be bidirectional</li>
    <li>Repeated polling: OK but not scalable</li>
    <li>HTTP request/response, but many hacks: <a href="https://en.wikipedia.org/wiki/Comet_(programming)" title="Wikipedia article about Comet">long polling / Comet</a></li>
    <li><a href="http://www.html5rocks.com/en/tutorials/eventsource/basics/" title="Server-sent Events article on HTML5 Rocks">EventSource</a> (aka Server-sent events): <a href="http://simpl.info/es" title="EventSource demo">demo</a></li>
    <li>WebSocket:
    <ul class='tight'>
      <li>more natural solution &mdash; it's bidirectional!</li>
      <li>supported by all browsers that support WebRTC, desktop and mobile</li>
      <li>use TLS: for security and to avoid proxy problems</li>
      <li>for more information: see Ilya Grigorik's <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch17.html#_http_upgrade_negotiation" title="Chapter from forthcoming O'Reilly book: High Performance Browser Networking">forthcoming O'Reilly chapter</a></li>
      <li>Peter Lubber's <a href="http://refcardz.dzone.com/refcardz/html5-websocket" title="WebSocket Cheat Sheet">WebSocket Cheat Sheet</a></li>
    </ul>
    </li>
  </ul>
  </article>
    <aside class="note">
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Signaling with Node and Socket.io</h2>
  </hgroup>
  <article>
  <ul>
    <li>Socket.io uses WebSocket with fallbacks</li>
    <li>Simple to exchange messages</li>
    <li>Built-in concept of 'rooms'</li>
  </ul>
  <p><a href="https://bitbucket.org/webrtc/codelab/complete/step5" title="Codelab walkthrough to build signaling server with Socket.io">Codelab</a></p>
  <p><a href="http://samdutton-nodertc.jit.su" title="Video chat application using Socket.io for signaling">Live example</a></p>
  </article>
    <aside class="note">
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Find me a candidate</h2>
  </hgroup>
  <article>
    <ol>
      <li>Caller creates RTCPeerConnection object</li>
      <li>If success, callback passed IceCandidate</li>
      <li>Caller sends IceCandidate to callee</li>
      <li>Callee creates a new remote IceCandidate, adds to remote description</li>
      <li>Ping!</li>
    </ol>
  </article>
    <aside class="note">
    <p>When the RTCPeerConnection object has been successfully created by the caller, the onIceCallback callback is called (in this example).</p>
      <p>This callback is passed a candidate object, which the caller then serialises and sends to the callee.</p>
      <p>On receipt of this message, the callee creates a new IceCandidate and calls processIceMessage() on it.</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Make me an offer</h2>
  </hgroup>
  <article>
    <ol>
      <li>RTCPeerConnection creates offer</li>
      <li>Caller sends offer</li>
      <li>Callee receives offer</li>
      <li>RTCPeerConnection creates answer</li>
      <li>Callee sends answer</li>
      <li>Caller receives answer</li>
      <li>Ping!</li>
    </ol>
  </article>
    <aside class="note">
      <ol>
        <li>The caller creates an RTCPeerConnection object and sets the local description.</li>
        <li>Caller uses RTCPeerConnection to create an offer. An offer is a blob -- a local session description in SDP format.</li>
        <li>The offer is serialised and communicated via the Signaling channel to the callee. </li>
        <li>When the callee gets the offer message, they create an RTCPeerConnection object and set the remote description from the offer.</li>
        <li>The callee then uses its RTCPeerConnection to creates an answer (passing it the offer) and sends that back to the caller.</li>
      </ol>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>An RTCSessionDescription</h2>
  </hgroup>
  <article>
<pre class="prettyprint" data-lang="ugh">
v=0
o=- 7614219274584779017 2 IN IP4 127.0.0.1
s=-
t=0 0
a=group:BUNDLE audio video
a=msid-semantic: WMS
m=audio 1 RTP/SAVPF 111 103 104 0 8 107 106 105 13 126
c=IN IP4 0.0.0.0
a=rtcp:1 IN IP4 0.0.0.0
a=ice-ufrag:W2TGCZw2NZHuwlnf
a=ice-pwd:xdQEccP40E+P0L5qTyzDgfmW
...
</pre>

<p>Want to know what all this SDP gobbledygook actually means?</p>
<p>Take a look at the <a href="http://datatracker.ietf.org/doc/draft-nandakumar-rtcweb-sdp/?include_text=1" title="IETF SDP examples">IETF examples</a>.</p>

  </article>
    <aside class="note">
    <p>In case you're wondering, the insides of a RTCSessionDescription look like this. While apps can manipulate this information for fine-grained control, we've tried to make it so that most apps don't need to worry about this.</p>
    <p>Bear in mind that WebRTC is designed so that the offer or answer can be tweaked before being set as the local or remote description, by editing the values in the SDP text. For example, the preferAudioCodec() function in apprtc.appspot.com can be used to set the default codec and bitrate. SDP is somewhat painful to manipulate with JavaScript, and there is discussion about whether future versions of WebRTC should use JSON instead, but there are some advantages to staying with SDP. </p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Making the connection</h2>
  </hgroup>
  <article class="flexbox vcenter">
<a href="http://www.w3.org/TR/webrtc/#simple-example" title="Simple W3C Signaling example">w3.org/TR/webrtc/#simple-example</a>
  </article>
    <aside class="note">
      <p>1. Fred creates an RTCPeerConnection object.
  2. Fred creates an offer (an SDP session description) with the RTCPeerConnection createOffer() method.
  3. Fred calls setLocalDescription() with the offer, to set local configuration.
  4. Fred stringifies the offer and uses a signaling mechanism to send it to Wilma.
  5. Wilma calls setRemoteDescription() with Fred's offer (the remote session description) so her RTCPeerConnection knows Fred's setup.
  6. Wilma also calls createAnswer() with the offer.
  7. The success callback for createAnswer() is passed a local session description for Wilma (which she'll uses as the answer) and she sets this as the local description by calling setLocalDescription().
  8. Wilma then uses the signaling mechanism to pass her answer session description back to Fred.
  9. Fred sets that as the remote session description using setRemoteDescription().</p>
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class="big"><a href="http://www.simpl.info/pc" title="Simple one-page RTCPeerConnection example">simpl.info/pc</a></div>
    </article>
    <aside class="note">
      <p>If you want to understand how WebRTC works, it's good to learn about RTCPeerConnection first, before you try to get your head around signaling mechanisms.</p>
      <p>This 'single page' demo does just that.</p>
      <p>It's very verbose: take a look at the console.</p>
      <p>Also take a look at chrome://webrtc-internals.</p>
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'><a href="http://apprtc.appspot.com" title="Canonical RTCPeerConnection videochat example">apprtc.appspot.com</a></div>
    </article>
    <aside class="note">
    <p>This is the best place to start with a fully featured WebRTC app: RTCPeerConnection, with signaling provided by XHR and DataChannel.</p>
    </aside>
</slide>

<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>RTCDataChannel</h2>
    <h3>Bidirectional communication of arbitrary data between peers</h3>
  </hgroup>
  <aside class="note">
    <p>The last API to talk about is RTCDataChannel.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Communicate arbitrary data</h2>
  </hgroup>
  <article class="fill flexbox vcenter">
  <div>

  <div style="float: left; width: 28%;">
    <img style="display: block; margin: 0 0 0.5em 0; position: relative; width: 100%;" src="images/jankInvadersScreenshot.jpg" alt="Game: caller" />
    <div style="background: #eee; font-family: 'Source Code Pro', 'Courier New', monospace; font-size: 0.5em; line-height: 1.2em; padding: 1em; white-space: pre;">onreceivemessage = handle(data);
...
var myData = [
  {
    id: "ship1";
    x: 24,
    y: 11,
    velocity: 7
  },
  ....
]
send(myData);
</div>
      </div>
      <div style="float: left; line-height: 2em; margin: 0 2em 0 2em; position: relative; text-align: center; top: 4em; width: 25%;">
      ‚Üí<br />
      RTCDataChannel<br />
      +<br />
      RTCPeerConnection<br />
      ‚Üê
      </div>

      <div style="float: left; width: 28%;">
        <div style="background: #eee; font-family: 'Source Code Pro', 'Courier New', monospace; font-size: 0.5em; line-height: 1.2em; margin: 0 0 1em 0; padding: 1em; white-space: pre;">onreceivemessage = handle(data);
...
var myData = [
  {
    id: "ship7";
    x: 19,
    y: 4,
    velocity: 18
  },
  ....
]
send(myData);
</div>
        <img style="display: block; width: 100%;" src="images/jankInvadersScreenshotReversed.jpg" alt="Game: callee" />
      </div>

  </div>
  </article>
  <aside class="note">
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>RTCDataChannel</h2>
  </hgroup>
  <article>
    <ul>
      <li>Same API as WebSockets</li>
      <li>Ultra-low latency</li>
      <li>Unreliable or reliable</li>
      <li>Secure</li>
    </ul>
  </article>
    <aside class="note">
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>RTCDataChannel API</h2>
  </hgroup>
  <article>
<pre class="prettyprint" data-lang="javascript">
var pc = new webkitRTCPeerConnection(servers,
  {optional: [{RtpDataChannels: true}]});

pc.ondatachannel = function(event) {
  receiveChannel = event.channel;
  receiveChannel.onmessage = function(event){
    document.querySelector("div#receive").innerHTML = event.data;
  };
};

sendChannel = pc.createDataChannel("sendDataChannel", {reliable: false});

document.querySelector("button#send").onclick = function (){
  var data = document.querySelector("textarea#send").value;
  sendChannel.send(data);
};
</pre>
  </article>
    <aside class="note">

    <p>SCTP is now available in Chrome 30 (flagged) and 31 (no flag).</p>
  <p>- optional reliable transfer, e.g. for file sharing (though in fact this has been accomplished over RTP, the old protocol for RTCDataChannel, which in practice is actually pretty reliable)</p>
  <p>- binary data</p>
  <p>- built-in flow control (flow/congestion control is built into SCTP, and bandwidth is managed not capped).</p>
    </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class="big"><a href="http://www.simpl.info/dc" title="Single page RTCDataChannel example">simpl.info/dc</a></div>
    </article>
    <aside class="note">
    </aside>
</slide>

<slide class="nobackground">
  <article class="fill flexbox vcenter">
    <a class="big" href="http://www.sharefest.me/" title="Sharefest app">Sharefest</a>
  </article>
  <aside class="note">
  </aside>
</slide>

<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>STUN and TURN</h2>
    <h3>P2P in the age of firewalls and NATs</h3>
  </hgroup>
  <aside class="note">
      <p>The other place where servers come into play is in figuring out how to route the peer-to-peer connection.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>An ideal world</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/noSTUNorTURN.png" alt="Data pathways between peers if there were no NATs or firewalls" style="position: relative; top: -30px">
  </article>
    <aside class="note">
      <p>In an ideal world, life would be simple - each endpoint could tell the other side its IP address, and a direct link could easily be established.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>The real world</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/firewall.png" alt="Data pathways showing firewalls" style="position: relative; top: -26px">

  </article>
   <aside class="note">
   <p>But in the age of NAT, this just isn't the case. Most users are behind what's called a NAT, which hands out a private IP address that can't be used for communication. Without a public address, there's no way to set up a peer-to-peer link.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>STUN</h2>
  </hgroup>
  <article>
    <ul>
      <li>Tell me what my public IP address is</li>
      <li>Simple server, cheap to run</li>
      <li>Data flows peer-to-peer</li>
    </ul>
  </article>
   <aside class="note">
   <p>To solve this, we use a technology called STUN. If we tell WebRTC about the location of a STUN server, it can ask the STUN server to tell it the right public address to use. The STUN server's job is simple - it just looks at where an incoming request is coming from, and sends that address back in the response.  WebRTC can then exchange that public address with the other side and use that to set up a direct link.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>STUN</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/stun.png" alt="Data pathways between peers using STUN" style="position: relative; top: -50px">
  </article>
      <aside class="note">
      <p>Here's a diagram of that in action. Each side asks its STUN server what its public address is, and then the peers can directly connect. Now, this technique usually works, but depending on the kind of NAT or firewall that is present, there are some cases where it doesn't.</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>TURN</h2>
  </hgroup>
  <article>
    <ul>
      <li>Provide a cloud fallback if peer-to-peer communication fails</li>
      <li>Data is sent through server, uses server bandwidth</li>
      <li>Ensures the call works in almost all environments</li>
    </ul>
  </article>
      <aside class="note">
      <p>The other technique that WebRTC can use is called TURN. A TURN server is essentially a server that relays the data an endpoint is trying to send to the other side. Because it has a public address already, it's easy to contact, so the connection always works, even in cases where the endpoint is behind a restrictive firewall or proxy. The downside is that all the data traffic has to go through the relay, so there's a nontrivial bandiwdth cost.</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>TURN</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/STUNandTURN.png" alt="Data pathways between peers using TURN" style="left: 1px; position: relative; top: -47px">
  </article>
   <aside class="note">
      <p>Here's a diagram of that in action. We tried to use STUN, but it didn't quite work right. So instead, each side contacted its own TURN server, and then data flow goes from each endpoint, through the TURN server, to the other side.</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>ICE</h2>
  </hgroup>
  <article>
     <ul>
        <li><a href="http://en.wikipedia.org/wiki/Interactive_Connectivity_Establishment" title="Wikipedia ICE article">ICE</a>: a framework for connecting peers</li>
      <li>Tries to find the best path for each call</li>
      <li>Vast majority of calls can use STUN (webrtcstats.com):
    </ul>
 <img src="images/icestats.png" alt="Data pathways between peers using TURN" style="left: 1px; position: relative; left: 200px">
  </article>
  <aside class="note">
      <p>On one hand, we have STUN, which is cheap, but doesn't always work. And on the other, we have TURN, which always works, but has an operational cost. Fortunately, we can get the best of both worlds. WebRTC uses a mechanism called ICE, Interactive_Connectivity_Establishment, to quickly figure out the best path. It tries all the possibilities in parallel and settles on the cheapest one that actually works.
          Based on some measurements done by the folks at WebRTCStats.com, about 86% of calls will work with just STUN, only 1 in 7 calls need to be routed through a TURN server.</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Deploying STUN/TURN</h2>
  </hgroup>
  <article>
     <ul>
        <li>stun.l.google.com:19302</li>
        <li>WebRTC stunserver, turnserver</li>
        <li><a href="http://code.google.com/p/rfc5766-turn-server" title="rfc5766-turn-server code and links to information">rfc5766-turn-server</a></li>
        <li> <a href="https://groups.google.com/forum/#!msg/discuss-webrtc/X-OeIUC0efs/XW5Wf7Tt1vMJ" title="">VM image for Amazon Web Services</a></li>
        <li>restund</li>
    </ul>
  </article>
  <aside class="note">
      <p>For basic testing, we run a public STUN server, and we also include source code for STUN and TURN servers in the WebRTC tree. For running a production STUN/TURN service, we recommend using rfc5766-turn-server, which has source code and AWS VM images, or restund, available as source code.</p>
    </aside>
</slide>

<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>Security</h2>
  </hgroup>
  <aside class="note">
  <p>One question that comes up about WebRTC is how security is handled.</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Security throughout WebRTC</h2>
  </hgroup>
  <article>
  <ul>
    <li>Mandatory encryption for media and data</li>
    <li>Secure UI, explicit opt-in</li>
    <li>Sandboxed, no plugins</li>
    <li><a href="http://www.ietf.org/proceedings/82/slides/rtcweb-13.pdf" title="Security Architecture slides">WebRTC Security Architecture</a></li>
  </ul>
  </article>
<aside class="note">
    <p>We've taken care to build security into WebRTC from the very beginning; all media and data is encrypted and protected, we require opt-in from the user before enabling their mic and camera, and WebRTC runs in the Chrome sandbox, which means that even if someone sends malicious data to WebRTC, the browser will be unaffected.</p>
    <p>However, signaling mechanisms aren't defined by WebRTC standards, so it's up to you make signaling secure. If an attacker manages to hijack signaling, they can stop sessions, redirect connections and record, alter or inject content.</p>
    <p>The most important factor in securing signaling is to use secure protocols, HTTPS and WSS, which ensure that messages cannot be intercepted unencrypted.  Also be careful not to broadcast signaling messages in a way that they can be accessed by other callers using the same signaling server.
</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Secure pathways</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/securePathways.png" alt="Secure pathways between peers" style="position: relative; top: -10px">
  </article>
  <aside class="note">
    <p>The only thing an app developer needs to do to ensure WebRTC security, is to use HTTPS when sending signaling data. In this diagram, the signaling pathways are protected using HTTPS, and the media and data are protected using the standard SRTP and DTLS mechanisms.</p>
  </aside>
</slide>

<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>Architectures</h2>
  </hgroup>
  <aside class="note">
  <p>Another question that developers have is how to architect their service.</p>
  </aside>
</slide>

<slide>
<hgroup>
    <h2>Peer to peer: one-to-one call</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/topologyOneToOne.png" alt="Topology diagram: one to one">
  </article>
    <aside class="note">
  <p>In the simplest case, a point-to-point call, the endpoints are directly connected.</p>
  </aside>
</slide>

<slide>
<hgroup>
    <h2>Mesh: small N-way call</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/topologyFullMesh.png" alt="Topology: full mesh" style='width: 450px;'>
  </article>
   <aside class="note">
  <p>For a multi-user call, we can extend that, and have every endpoint connect to every other endpoint. Note that this means that each endpoint is sending multiple copies of the data, which leads to higher CPU and network utilization. Depending on what kind of media you are sending, this becomes unworkable after a certain number of participants, especially if you have mobile clients in the mix.</p>
  </aside>
</slide>

<slide>
<hgroup>
    <h2>Star: medium N-way call</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/topologyOneToThree.png" alt="Topology diagram: one to three" style='width: 450px;'>
  </article>
     <aside class="note">
  <p>Another option is to designate one endpoint as a "focus", and have that endpoint handle the task of distributing data to the others; the application can choose the most capable endpoint to serve as the focus. This tends to work better than a mesh, but still runs into trouble when handling multiple HD video streams.</p>
  </aside>
</slide>

<slide>
<hgroup>
    <h2>MCU: large N-way call</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <img src="images/topologyMCU.png" alt="Topology: MCU" style='width: 600px;'>
  </article>
    <aside class="note">
  <p>The most robust solution is to use what's called a MCU, or multipoint control unit. This is a server that's made specifically to do distribution of media, and can handle large numbers of participants; it can also do smart things like selective stream forwarding, mixing of the audio or video, or recording.</p>
  </aside>
</slide>

<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>Beyond browsers</h2>
  </hgroup>
  <aside class="note">
      <p></p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Phones and more</h2>
  </hgroup>
  <article>
  <ul>
    <li>Easy to interoperate with non-browser devices</li>
    <ul>
    <li><a href="https://code.google.com/p/sipml5/" title="">sipML5</a> open source JavaScript SIP client</li>
      <li><a href="http://phono.com/" title="Phono SDK site">Phono</a> open source JavaScript phone API</li>
      <li><a href="http://zingaya.com/product/" title="Zingaya SDK site">Zingaya</a> embeddable phone widget</li>
      </ul>
    </ul>
  </article>
    <aside class="note">
      <p>WebRTC has been designed with standards in mind, which means it's easy for it to communicate with non-WebRTC devices, like phones. There are a bunch of libraries out there that allow you to do this with just a few lines of code. Let's take a look at a demo from Zingaya.'</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>Telephony</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <div class='big'><a href="http://demos.zingaya.com/webrtc-pstn/" title="Zingaya WebRTC PSTN demo">Zingaya PSTN</a></div>
  </article>
  <aside class='note'>
    <p>631-403-9000</p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Tethr</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <a href="http://tethr.tumblr.com" title="WebRTC API documentation"><img style="height: 100%" src="images/tethr.jpg" alt="Tethr in action at Google I/O 2012" /></a>
  </article>
    <aside class="note">
      <p>At Google I/O last year Voxeo demonstrated a framework for disaster communications:</p>
      <p>Set up an OpenBTS cell to enable communications between feature phones and computers via WebRTC.</p>
      <p>Telephone communication without a carrier!</p>
    </aside>
</slide>



<slide class="segue dark nobackground">
  <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside>
  <hgroup class="auto-fadein">
    <h2>Building a WebRTC app</h2>
  </hgroup>
  <aside class="note">
    <p>We have tools to help you!</p>
  </aside>
</slide>

<slide class="nobackground">
 <hgroup>
    <h2>chrome://webrtc-internals</h2>
    </hgroup>
     <article class="fill flexbox vcenter">
    <img src="images/internals.png" alt="chrome://webrtc-internals screenshot" style="border: 1px solid #4444; position: relative; top: -48px; left: -54px;" />
</article>
    <aside class="note">
    <p>One tool</p>
    </aside>
</slide>

<slide class="nobackground">
<hgroup>
    <h2><a href="https://code.google.com/p/webrtc/source/browse/trunk/samples/apprtc/js/base/adapter.js">adapter.js</a></h2>
    </hgroup>
    <article>
    <p>Lets you use the same code in all browsers:</p>
      <ul>
        <li>Removes vendor prefixes</li>
        <li>Abstracts Chrome/Firefox differences</li>
        <li>Minimizes effects of spec churn</li>
       </ul>
  </article>
  <aside class="note">
  </aside>
</slide>

<slide class="nobackground red">
    <article class="fill flexbox vcenter">
      <div class="big"><strong>This is doing my head in.</strong></div>
    </article>
    <aside class="note">
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>JavaScript frameworks</h2>
  </hgroup>
  <article>
    <ul>
      <li>Video chat:</li>
      <ul>
      <li><a href="https://github.com/henrikjoreteg/SimpleWebRTC" title="SimpleWebRTC github repository">SimpleWebRTC</a></li>
      <li><a href="https://github.com/priologic/easyrtc" title="easyRTC github repository">easyRTC</a></li>
      <li><a href="https://github.com/webRTC/webRTC.io" title="webRTC.io github repository">webRTC.io</a></li>
      </ul>
      <li>Peer-to-peer data:</li>
      <ul>
      <li><a href="http://peerjs.com/" title="PeerJS site">PeerJS</a></li>
      <li><a href="https://github.com/peer5/sharefest" title="Sharefest github repository">Sharefest</a></li>
      </ul>
    </ul>
  </article>
    <aside class="note">
    </aside>
</slide>


<!-- <slide>
  <hgroup>
    <h2>SimpleWebRTC</h2>
    <h3>HTML</h3>
  </hgroup>
  <article>
    <pre class="prettyprint" data-lang="html">
&lt;!DOCTYPE html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;script src="http://simplewebrtc.com/latest.js"&gt;&lt;/script&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;div id="localVideo"&gt;&lt;/div&gt;
        &lt;div id="remoteVideos"&gt;&lt;/div&gt;
    &lt;/body&gt;
&lt;/html&gt;
</pre>
  </article>
</slide> -->

<slide>
  <hgroup>
    <h2>SimpleWebRTC</h2>
    <h3>Easy peer-to-peer video and audio</h3>
  </hgroup>
  <article>
    <pre class="prettyprint" data-lang="javascript">
var webrtc = new WebRTC({
  localVideoEl: 'localVideo',
  remoteVideosEl: 'remoteVideos',
  autoRequestMedia: true
});

webrtc.on('readyToCall', function () {
    webrtc.joinRoom('My room name');
});
</pre>
  </article>
</slide>


<slide>
  <hgroup>
    <h2>PeerJS</h2>
    <h3>Easy peer-to-peer data</h3>
  </hgroup>
  <article>
    <pre class="prettyprint" data-lang="javascript">
var peer = new Peer('someid', {key: 'apikey'});
peer.on('connection', function(conn) {
  conn.on('data', function(data){
    // Will print 'hi!'
    console.log(data);
  });
});

// Connecting peer
var peer = new Peer('anotherid', {key: 'apikey'});
var conn = peer.connect('someid');
conn.on('open', function(){
  conn.send('hi!');
});

</pre>
  </article>
</slide>

<slide>
  <hgroup>
    <h2>Services</h2>
  </hgroup>
  <article>
  <ul><li>Complete video services:</li>
  <ul>
    <li><a href="http://www.tokbox.com/" title="Tokbox site">OpenTok</a> (acquired by Telefonica Digital)</li>
    <li><a href="http://www.vline.com/" title="vLine">vLine</a></li>
  </ul>
    </ul>
        <img src="images/networkmap.png" alt="Data center locations" style="" />
  </article>
    <aside class="note">
      <p>Javascript frameworks make writing apps easier, but they don't handle some of the more complicated aspects, like signaling and TURN servers. Fortunately, there are a couple turnkey WebRTC services out there that take care of all of this for you. You can sign up for these services, get an API key, and then make calls using their deployed infrastructure; they also offer UI components that can easily be integrated into your application.</p>
    </aside>
</slide>

<!-- <slide>
  <hgroup>
    <h2>Don't forget the C++!</h2>
  </hgroup>
  <article class="flexbox vcenter">
    <a href="http://www.webrtc.org/reference/webrtc-internals" title="WebRTC API documentation">webrtc.org/reference/webrtc-internals</a>
  </article>
    <aside class="note">
      <p>The WebRTC C++ APIs mean you can build a WebRTC client on a server -- check out the example in the webrtc.org source repository.</p>
      <p>Also a Qt/C++ demo on YouTube.</p>
    </aside>
</slide> -->

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class='big'><a href="http://apprtc.appspot.com/?r=io&hd=true" title="">Chris Wilson <strong>LIVE!</strong></a></div>
    </article>
    <aside class="note">
      <p>Before we wrap up, we have a special treat for you. I'd like to introduce Chris Wilson - a colleague of ours, who worked on the original Mosaic browser, and is a bit of a musician. Chris is joining us today, via WebRTC, to show off our support for HD video, and crystal clear, fullband audio. Take it away, Chris!</p>
    </aside>
</slide>

<slide>
  <hgroup>
    <h2>More Information</h2>
  </hgroup>
  <article>
  <ul>
    <li>Justin Uberti: <a href="http://www.youtube.com/watch?v=E8C8ouiXHHk" title="Video of Google I/O 2012 presentation">Google I/O presentation video</a></li>
    <li>Cullen Jennings video: <a href="http://vimeo.com/47682405" title="IETF and W3C standardisation discussion">HTML5 WebRTC</a></li>
    <li>HTML5 Rocks:</li>
    <ul>
      <li><a href="http://www.html5rocks.com/en/tutorials/getusermedia/intro/" title="HTML5 Rocks article about getUserMedia">Capturing audio and video in HTML5</a></li>
      <li><a href="http://www.html5rocks.com/en/tutorials/webrtc/basics/" title="HTML5 Rocks article about WebRTC">Getting Started With WebRTC</a></li>
      <li><a href="http://www.html5rocks.com/en/search?q=webrtc" title="HTML5 content tagged WebRTC">Updates</a></li>
    </ul>
    <li>...and a book: <a href="http://www.webrtcbook.com" title="WebRTC ebook download">webrtcbook.com</a></li>
  </ul>
  </article>
<aside class="note">
    <p></p>
  </aside>
</slide>

<slide>
  <hgroup>
    <h2>Contact Us</h2>
  </hgroup>
  <article>
  <ul>
    <li><a href="webrtc.org" title="WebRTC project website">webrtc.org</a></li>
    <li><a href="https://groups.google.com/forum/?fromgroups#!forum/discuss-webrtc" title="WebRTC discussion group">discuss-webrtc</a></li>
    <li><a href="https://plus.sandbox.google.com/113817074606039822053/posts" title="WebRTC on Google+">+webrtc</a></li>
      <li><a href="https://twitter.com/webrtc" title="WebRTC on Twitter">@webrtc</a></li>
      <li>  <a href="http://www.crbug.com/new" title="Report Chrome bugs and feature requests">crbug.com/new</a></li>
  </ul>
  </article>
<aside class="note">
    <p>webrtc.org has a blog, links to demos, documentation and links to code repositories</p>
    <p>...and follow Justin Uberti and Serge Lachapelle on Google+</p>
  </aside>
</slide>


<slide class="segue dark quote nobackground">
  <aside class="gdbar right bottom"><img src="images/google_developers_icon_128.png"></aside>
  <article class="flexbox vleft auto-fadein">
    <q>WebRTC and HTML5 could enable the same transformation for real-time communications that the original browser did for information.</q>
    <div class="author">
      Phil Edholm<br>
      &mdash; NoJitter
    </div>
  </article>
  <aside class="note">
    <section>
      <ul>
        <li>So now it's your turn. We've built this technology into the web so that any developer can take advantage of real-time communications, and we can't wait to see what you do with it.</li>
      </ul>
    </section>
  </aside>
</slide>

<slide class="nobackground">
    <article class="fill flexbox vcenter">
      <div class="big"><a href="http://io13webrtc.appspot.com" title="These slides online">io13webrtc.appspot.com</a></div>
    </article>
    <aside class="note">
    Once again, the link to the slides.
    </aside>
</slide>

<slide class="thank-you-slide segue nobackground">
  <aside class="gdbar right"><img src="images/google_developers_icon_128.png"></aside>
  <article class="flexbox vleft auto-fadein">
    <h2>&lt;Thank You!&gt;</h2>
  </article>
  <p class="auto-fadein" data-config-contact>
    <!-- populated from slide_config.json -->
  </p>
</slide>

<slide class="logoslide dark nobackground">
  <article class="flexbox vcenter">
    <span><img src="images/google_developers_logo_white.png"></span>
  </article>
</slide>

<slide class="backdrop"></slide>

</slides>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-41621713-1', 'io13webrtc.appspot.com');
  ga('send', 'pageview');

</script>

<!--[if IE]>
  <script src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js"></script>
  <script>CFInstall.check({mode: 'overlay'});</script>
<![endif]-->
</body>
</html>
